import logging
import requests
from bs4 import BeautifulSoup
import pandas as pd
import pickle
import os
import time
from geopy.geocoders import Nominatim
from io import StringIO
import feedparser
from datetime import datetime
import pytz
import streamlit as st


# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.DEBUG)


GEOLOC_CACHE_FILE = 'geolocation_cache.pkl'


# ìŠ¤ë ˆë“œ í’€ ì‹¤í–‰ì ì´ˆê¸°í™”
# executor = concurrent.futures.ThreadPoolExecutor()


# Set verbose if needed
# globals.set_debug(True)


# # # # # # # # # # # #
# ì„¸ì…˜ ìºì‹œ ì²˜ë¦¬
# # # # # # # # # # # #


if "geolocations" not in st.session_state:
    if os.path.exists(GEOLOC_CACHE_FILE):
        # ìºì‹œ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¨ë‹¤.
        with open(GEOLOC_CACHE_FILE, 'rb') as f:
            st.session_state.geolocations = pickle.load(f)
            logging.info('ìºì‹œ íŒŒì¼ ë¡œë”© ì™„ë£Œ')
            logging.info(st.session_state.geolocations)
    else:
        # ìºì‹œ íŒŒì¼ì´ ì—†ìœ¼ë©´
        st.session_state.geolocations = dict()  # ì§€ì—­ë³„ ìœ„ê²½ë„ dictë¥¼ ëª¨ì•„ë‘” dict
        logging.info('ìºì‹œ íŒŒì¼ ì—†ìŒ.')


# # # # # # # # # # # #
# ì´ë¯¸ì§€ í•´ì„ ì²´ì¸ #
# # # # # # # # # # # #


# # # # # # # # # #
# êµ¬ê¸€ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° #
# # # # # # # # # #


def get_google_outage_news(keyword_):

    url = f"https://news.google.com/rss/search?q={keyword_ + ' ' + and_keyword}+when:{search_hour}h"
    url += f'&hl=en-US&gl=US&ceid=US:en'
    url = url.replace(' ', '%20')

    title_list = []
    source_list = []
    pubtime_list = []
    link_list = []

    try:
        res = requests.get(url)  # , verify=False)
        st.write('ì›ë³¸ ë§í¬: ' + url)

        if res.status_code == 200:
            datas = feedparser.parse(res.text).entries
            for data in datas:
                title = data.title
                logging.info('êµ¬ê¸€ë‰´ìŠ¤ì œëª©(ì›ë³¸): ' + title)

                minus_index = title.rindex(' - ')
                title = title[:minus_index].strip()

                # ê¸°ì‚¬ ì œëª©ì— ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ë„˜ê¸´ë‹¤.
                # if keyword_ not in title or and_keyword not in title:
                #     continue

                title_list.append(title)
                source_list.append(data.source.title)
                link_list.append(data.link)

                pubtime = datetime.strptime(data.published, "%a, %d %b %Y %H:%M:%S %Z")
                # GMT+9 (Asia/Seoul)ìœ¼ë¡œ ë³€ê²½
                gmt_plus_9 = pytz.FixedOffset(540)  # 9 hours * 60 minutes = 540 minutes
                pubtime = pubtime.replace(tzinfo=pytz.utc).astimezone(gmt_plus_9)

                pubtime_str = pubtime.strftime('%Y-%m-%d %H:%M:%S')
                pubtime_list.append(pubtime_str)

        else:
            logging.error("Google ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨! Error Code: " + str(res.status_code))
            logging.error(str(res))
            return None

    except Exception as e:
        logging.error(e)
        logging.error("Google ë‰´ìŠ¤ RSS í”¼ë“œ ì¡°íšŒ ì˜¤ë¥˜ ë°œìƒ!")
        return None

    # ê²°ê³¼ë¥¼ dict í˜•íƒœë¡œ ì €ì¥
    result = {'ì œëª©': title_list, 'ì–¸ë¡ ì‚¬': source_list, 'ë°œí–‰ì‹œê°„': pubtime_list, 'ë§í¬': link_list}

    df = pd.DataFrame(result)
    return df


def display_news_df(ndf):
    if news_df is None or len(news_df) == 0:
        st.write('ê²€ìƒ‰ëœ ë‰´ìŠ¤ ì—†ìŠµë‹ˆë‹¤.')
        return

    st.write('ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼')
    st.divider()

    for i, row in ndf.iterrows():
        st.header(row['ì œëª©'])
        st.write('ì–¸ë¡ ì‚¬: ' + row['ì–¸ë¡ ì‚¬'])
        st.write('ë°œí–‰ì‹œê°: ' + row['ë°œí–‰ì‹œê°„'])
        st.write(row['ë§í¬'])
        st.divider()


# # # # # # # # # # # # # # #
# down ì°¨íŠ¸, ì§€ë„ ë§í¬ ë°›ì•„ì˜¤ê¸°
# # # # # # # # # # # # # # #


def get_service_chart_mapdf(service_name):
    # url = "https://istheservicedown.com/problems/disney-plus"
    url = "https://istheservicedown.com/problems/" + service_name

    req_ = requests.Session()
    response_ = req_.get(url,
                         headers={'User-Agent': 'Popular browser\'s user-agent', })

    soup_ = BeautifulSoup(response_.content, 'html.parser')

    # ìƒíƒœ
    status_ = soup_.find('p').text

    # ì°¨íŠ¸ ì£¼ì†Œ
    chart_url_ = soup_.find(id="chart-img")['src']

    # map ì£¼ì†Œ
    sub_url = soup_.find(title='Live Outage Map')['href']
    if sub_url:
        map_url_ = 'https://istheservicedown.com' + sub_url

        response_ = req_.get(map_url_,
                             headers={'User-Agent': 'Popular browser\'s user-agent', })

        soup_ = BeautifulSoup(response_.content, 'html.parser')
        table_html = soup_.find('table', class_="table table-striped table-condensed")  # , id_='status-table')
        map_df_ = pd.read_html(StringIO(str(table_html)))[0]
    else:
        map_df_ = None

    return status_, chart_url_, map_df_


# # # # # # # # # #
# ìœ„ê²½ë„ ë°›ì•„ì˜¤ê¸°
# # # # # # # # # #


def save_cache(loc, lat, lon):
    st.session_state.geolocations[loc] = {'lat': lat, 'lon': lon}
    # ìƒˆë¡œìš´ ìœ„ê²½ë„ ì •ë³´ë¥¼ ìºì‹œ íŒŒì¼ì— ì €ì¥
    with open(GEOLOC_CACHE_FILE, 'wb') as f_:
        pickle.dump(st.session_state.geolocations, f_)
        logging.info('ìºì‹œ íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ')


def load_cache(loc):
    return st.session_state.geolocations.get(loc)


def get_geo_location(map_df_):
    geolocator = Nominatim(user_agent="jason")
    map_df_['lat'] = None
    map_df_['lon'] = None
    map_df_['color'] = '#ff000077'  # ë¹¨ê°•, ì‚´ì§ íˆ¬ëª….

    for i, row in map_df_.iterrows():
        # ì„¸ì…˜ ìºì‹œë¥¼ ë¨¼ì € ì‚´í´ë³¸ë‹¤.
        cache = load_cache(row['Location'])
        if cache:
            logging.info('cache hit! - ' + row['Location'])
            map_df_.loc[i, 'lat'] = cache['lat']
            map_df_.loc[i, 'lon'] = cache['lon']
            continue

        # ì„¸ì…˜ ìºì‹œì— ì—†ìœ¼ë©´ ìœ„ê²½ë„ apië¥¼ ì¨ì„œ ë¶ˆëŸ¬ì˜¨ë‹¤.
        geo = geolocator.geocode(row['Location'])

        if geo:
            map_df_.loc[i, 'lat'] = geo.latitude
            map_df_.loc[i, 'lon'] = geo.longitude
            save_cache(row['Location'], geo.latitude, geo.longitude)
        else:
            # retry
            geo = geolocator.geocode(row['Location'].split(',')[0])

            if geo:
                map_df_.loc[i, 'lat'] = geo.latitude
                map_df_.loc[i, 'lon'] = geo.longitude
                save_cache(row['Location'], geo.latitude, geo.longitude)
            else:
                # retryê¹Œì§€ ì‹¤íŒ¨í•  ê²½ìš°.
                logging.error('Geo ERROR!!! :' + str(geo))

        time.sleep(0.2)
    return map_df_


# # # # # # # # # #
# íšŒì‚¬ ëª©ë¡ ë°›ì•„ì˜¤ê¸°
# # # # # # # # # #


req = requests.Session()
response = req.get('https://istheservicedown.com/companies',
                   headers={'User-Agent': 'Popular browser\'s user-agent'})

soup = BeautifulSoup(response.content, 'html.parser')

companies_html_list = soup.find_all('a', class_='b-lazy-bg')
companies_list = []

for company in companies_html_list:
    code = company['href'].split('/')[-1]
    name = company.h3.text
    logging.debug(code + ' (' + name + ')')
    companies_list.append((code, name))

logging.info('Total companies count:' + str(len(companies_list)))


# # # # # # # # # #
# ì›¹ í˜ì´ì§€ êµ¬ì„±
# # # # # # # # # #


st.set_page_config(layout="wide")
# st.title('ë‰´ìŠ¤ ê²€ìƒ‰ ë´‡')


# ì‚¬ì´ë“œë°” êµ¬ì„±
service_code_name = st.sidebar.selectbox(
    "ê²€ìƒ‰ì„ ì›í•˜ëŠ” ì„œë¹„ìŠ¤ëŠ”?",
    [a + '/' + b for a, b in companies_list],
    index=None,
    placeholder="ì„œë¹„ìŠ¤ ì´ë¦„ ì„ íƒ...",
)

another_service = st.sidebar.text_input("ëª©ë¡ì— ì—†ì„ ê²½ìš° ì—¬ê¸° ì„œë¹„ìŠ¤ëª…ì„ ì ìœ¼ì„¸ìš”! (ì˜ì–´ë¡œ)", )

search_hour = st.sidebar.number_input('ìµœê·¼ ëª‡ì‹œê°„ì˜ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í• ê¹Œìš”?', value=1, format='%d')

and_keyword = st.sidebar.text_input("ë‰´ìŠ¤ ê²€ìƒ‰ ì¶”ê°€ í‚¤ì›Œë“œ", value='outage', disabled=True)


if service_code_name:
    selected_code = service_code_name.split('/')[0]
    selected_name = service_code_name.split('/')[1]

    st.title(selected_name)
    # st.write("ì„ íƒ ì„œë¹„ìŠ¤ ì½”ë“œ: ", selected_code)

    with st.spinner('ì„œë¹„ìŠ¤ ìƒíƒœ ì¡°íšŒì¤‘...'):
        status, chart_url, map_df = get_service_chart_mapdf(selected_code)

        # ìƒíƒœ
        if 'No problem' in status:
            color = 'green'
        elif status == 'Some problems detected':
            color = 'orange'
        else:  # 'Problems detected':
            color = 'red'

        st.header(f'ğŸ‘‰ :{color}[{status}]')

    st.divider()

    st.write('Problems reported in the last 24 hours')

    # HTML iframe íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ì‚¬ì´íŠ¸ ì„ë² ë“œ
    chart_iframe_html = f"""
    <iframe src={chart_url} width="800" height="400" frameborder="0"></iframe>
    """
    st.markdown(chart_iframe_html, unsafe_allow_html=True)

    st.divider()

    with st.spinner('ì„œë¹„ìŠ¤ ë§µ êµ¬ì„±ì¤‘...'):
        map_df = get_geo_location(map_df)

        st.write('Most affected locations in the past 15 days')

        # ì§€ë„ ê·¸ë¦¬ê¸°
        drawing_df = map_df.dropna()

        max_report = drawing_df['Reports'].max()
        multiple = 50000
        if max_report >= 5000:
            multiple = 100
        elif max_report >= 2000:
            multiple = 250
        elif max_report >= 1000:
            multiple = 500
        elif max_report >= 500:
            multiple = 1000
        elif max_report >= 100:
            multiple = 5000
        elif max_report >= 50:
            multiple = 10000

        drawing_df['Reports'] = drawing_df['Reports'] * multiple
        st.map(drawing_df,
            latitude='lat',
            longitude='lon',
            size='Reports',
            color='color')

        st.write(map_df)

        # map í˜ì´ì§€ ì¶œë ¥.
        # map_iframe_html = f"""
        # <iframe src={map_url} width="800" height="600" frameborder="0"></iframe>
        # """
        # st.markdown(map_iframe_html, unsafe_allow_html=True)

    st.divider()

    with st.spinner('í•´ì™¸ì–¸ë¡  ê²€ìƒ‰ì¤‘...'):
        news_df = get_google_outage_news(selected_name)
        # st.write(news_df)
        display_news_df(news_df)


if another_service and not service_code_name:
    st.title(another_service)

    with st.spinner('í•´ì™¸ì–¸ë¡  ê²€ìƒ‰ì¤‘...'):
        news_df = get_google_outage_news(another_service)
        # st.write(news_df)
        display_news_df(news_df)


# ë©”ì¸ í˜ì´ì§€ êµ¬ì„±

chat_placeholder = st.empty()
